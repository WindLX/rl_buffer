# RL-Buffer

[![PyTorch](https://img.shields.io/badge/PyTorch-%23EE4C2C.svg?style=for-the-badge&logo=PyTorch&logoColor=white)](https://pytorch.org/)
[![Python Version](https://img.shields.io/badge/Python-3.12%2B-blue?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg?style=for-the-badge)](https://opensource.org/licenses/MIT)

[English](README.md) | [ä¸­æ–‡](README_zh-cn.md) 

**`rl-buffer`** æ˜¯ä¸€ä¸ªä¸ºç°ä»£å¼ºåŒ–å­¦ä¹ ç®—æ³•è®¾è®¡çš„é«˜æ€§èƒ½ã€é«˜çµæ´»æ€§ã€åŠŸèƒ½ä¸°å¯Œçš„ PyTorch ç¼“å†²åº“ã€‚å®ƒæä¾›äº†ä¸“ä¸º Off-Policy å’Œ On-Policy ç®—æ³•ä¼˜åŒ–çš„æ ¸å¿ƒç»„ä»¶ï¼Œå¹¶é›†æˆäº†å¼ºå¤§çš„ç»Ÿè®¡è·Ÿè¸ªåŠŸèƒ½ï¼Œæ—¨åœ¨åŠ é€Ÿæ‚¨çš„RLç ”ç©¶å’Œå¼€å‘æµç¨‹ã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

-   **é«˜æ€§èƒ½**: æ‰€æœ‰ç¼“å†²æ“ä½œéƒ½åŸºäº `torch.Tensor`ï¼Œæ— ç¼æ”¯æŒ GPU åŠ é€Ÿï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘äº†æ•°æ®åœ¨ CPU å’Œ GPU ä¹‹é—´çš„ä¼ è¾“ã€‚
-   **ä¸¤ç§æ ¸å¿ƒç¼“å†²**:
    -   `ReplayBuffer`: ä¸º DQN, SAC, DDPG ç­‰ Off-Policy ç®—æ³•è®¾è®¡çš„æ ‡å‡†ç»éªŒå›æ”¾ç¼“å†²ã€‚
    -   `RolloutBuffer`: ä¸º PPO, A2C ç­‰ On-Policy ç®—æ³•è®¾è®¡ï¼Œå†…ç½® **GAE (Generalized Advantage Estimation)** è®¡ç®—ã€‚
-   **é«˜çµæ´»æ€§**:
    -   **è‡ªå®šä¹‰æ•°æ®ç±»å‹**: æ”¯æŒä¸ºè§‚æµ‹ (`obs`) å’ŒåŠ¨ä½œ (`action`) æŒ‡å®š `dtype`ï¼ˆä¾‹å¦‚ï¼Œ`torch.uint8` ç”¨äºå›¾åƒè§‚æµ‹ï¼‰ï¼Œæœ‰æ•ˆä¼˜åŒ–å†…å­˜/æ˜¾å­˜å ç”¨ã€‚
    -   **å¤šç¯å¢ƒå¹¶è¡Œ**: åŸç”Ÿæ”¯æŒä¸å¤šä¸ªå¹¶è¡Œç¯å¢ƒï¼ˆ`num_envs`ï¼‰çš„äº¤äº’ã€‚
-   **å¼ºå¤§çš„ç»Ÿè®¡ä¸è¯Šæ–­**:
    -   å†…ç½® `StatsTracker`ï¼Œå¯è½»æ¾è·Ÿè¸ªå’Œè®°å½•å›åˆçš„å¥–åŠ±ã€é•¿åº¦ã€è‡ªå®šä¹‰æŒ‡æ ‡ï¼ˆå¦‚æ¸¸æˆå¾—åˆ†ï¼‰ä»¥åŠç»ˆæ­¢åŸå› ã€‚
    -   è‡ªåŠ¨è®¡ç®—å‡å€¼ã€æ ‡å‡†å·®ã€æœ€å¤§/æœ€å°å€¼ç­‰ç»Ÿè®¡æ•°æ®ï¼Œä¾¿äºæ—¥å¿—è®°å½•å’Œåˆ†æã€‚
-   **ç°ä»£åŒ– API**:
    -   `RolloutBuffer` çš„ `get()` æ–¹æ³•è¿”å›ä¸€ä¸ª**ç”Ÿæˆå™¨**ï¼Œå®ç°äº†é«˜æ•ˆçš„å†…å­˜åˆ©ç”¨ï¼Œç‰¹åˆ«é€‚åˆäºå¤§å‹ç¼“å†²å’Œå¤šæ¬¡è¿­ä»£ï¼ˆepochï¼‰çš„è®­ç»ƒã€‚
    -   æ¸…æ™°ã€ç»è¿‡å•å…ƒæµ‹è¯•çš„æ¥å£ï¼Œæ˜“äºé›†æˆåˆ°ç°æœ‰é¡¹ç›®ä¸­ã€‚

## ğŸš€ å®‰è£…

æœ¬é¡¹ç›®ä½¿ç”¨ `uv` è¿›è¡ŒåŒ…ç®¡ç†ã€‚

1.  **åˆ›å»ºå¹¶æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ**:
    ```bash
    uv venv
    source .venv/bin/activate
    ```

2.  **å®‰è£…é¡¹ç›®åŠå…¶ä¾èµ–**:
    è¦è¿›è¡Œå¼€å‘å’Œè¿è¡Œæµ‹è¯•ï¼Œè¯·å®‰è£… `dev` ä¾èµ–é¡¹ã€‚
    ```bash
    uv pip install -e ".[dev]"
    ```
    å¦‚æœåªç”¨äºé¡¹ç›®é›†æˆï¼Œå®‰è£…åŸºç¡€ä¾èµ–å³å¯ï¼š
    ```bash
    uv pip install -e .
    ```

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹ 1: `ReplayBuffer` (ç”¨äº Off-Policy ç®—æ³•)

è¿™ä¸ªä¾‹å­æ¨¡æ‹Ÿäº†ä¸€ä¸ªå…¸å‹çš„ Off-Policy ç®—æ³•ï¼ˆå¦‚ SACï¼‰çš„æ•°æ®æ”¶é›†å’Œé‡‡æ ·è¿‡ç¨‹ã€‚

```python
import torch
from rl_buffer.replay_buffer import ReplayBuffer
from rl_buffer.stats_tracker import StatsTracker

# 1. ç¯å¢ƒå’Œç¼“å†²é…ç½®
NUM_ENVS = 4
BUFFER_SIZE = 1000
BATCH_SIZE = 64
OBS_SHAPE = (16,)
ACTION_SHAPE = (4,)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 2. åˆå§‹åŒ– StatsTracker å’Œ ReplayBuffer
stats_tracker = StatsTracker(num_envs=NUM_ENVS)
replay_buffer = ReplayBuffer(
    buffer_size=BUFFER_SIZE,
    num_envs=NUM_ENVS,
    obs_shape=OBS_SHAPE,
    action_shape=ACTION_SHAPE,
    stats_tracker=stats_tracker,
    device=DEVICE
)

print(f"--- Populating ReplayBuffer on {DEVICE} ---")

# 3. æ¨¡æ‹Ÿæ•°æ®æ”¶é›†å¾ªç¯
for step in range(BUFFER_SIZE):
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
    obs = torch.randn(NUM_ENVS, *OBS_SHAPE)
    action = torch.randn(NUM_ENVS, *ACTION_SHAPE)
    reward = torch.randn(NUM_ENVS)
    # æ¨¡æ‹Ÿå¤§çº¦æ¯ 100 æ­¥æœ‰ä¸€ä¸ªç¯å¢ƒç»“æŸ
    done = torch.rand(NUM_ENVS) < 0.01
    next_obs = torch.randn(NUM_ENVS, *OBS_SHAPE)

    replay_buffer.add(obs, action, reward, done, next_obs)

print(f"Buffer populated. Current size: {len(replay_buffer)}")

# 4. ä»ç¼“å†²ä¸­é‡‡æ ·ä¸€ä¸ªæ‰¹æ¬¡
if len(replay_buffer) > BATCH_SIZE:
    print(f"\n--- Sampling a batch of size {BATCH_SIZE} ---")
    batch = replay_buffer.get(batch_size=BATCH_SIZE)
    print(f"Sampled observations shape: {batch.observations.shape}")
    print(f"Sampled actions shape: {batch.actions.shape}")
    print(f"Sampled rewards shape: {batch.rewards.shape}")

# 5. æŸ¥çœ‹ç»Ÿè®¡ç»“æœ
print("\n--- Episode Statistics ---")
stats = stats_tracker.get_statistics()
for key, value in stats.items():
    print(f"{key}: {value:.4f}")

```

### ç¤ºä¾‹ 2: `RolloutBuffer` (ç”¨äº On-Policy ç®—æ³•)

è¿™ä¸ªä¾‹å­æ¨¡æ‹Ÿäº† On-Policy ç®—æ³•ï¼ˆå¦‚ PPOï¼‰çš„å®Œæ•´æµç¨‹ï¼šæ”¶é›†ä¸€ä¸ª rollout -> è®¡ç®— GAE -> åœ¨å¤šä¸ª epoch ä¸­è¿­ä»£è®­ç»ƒã€‚

```python
import torch
from rl_buffer.rollout_buffer import RolloutBuffer
from rl_buffer.stats_tracker import StatsTracker

# 1. ç¯å¢ƒå’Œç¼“å†²é…ç½®
NUM_ENVS = 4
BUFFER_SIZE = 256 # On-policy ä¸­é€šå¸¸ç§°ä¸º n_steps
BATCH_SIZE = 64
N_EPOCHS = 4
OBS_SHAPE = (16,)
ACTION_SHAPE = (1,)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 2. åˆå§‹åŒ– StatsTracker å’Œ RolloutBuffer
stats_tracker = StatsTracker(num_envs=NUM_ENVS)
rollout_buffer = RolloutBuffer(
    buffer_size=BUFFER_SIZE,
    num_envs=NUM_ENVS,
    obs_shape=OBS_SHAPE,
    action_shape=ACTION_SHAPE,
    stats_tracker=stats_tracker,
    device=DEVICE
)

print(f"--- Collecting one rollout of {BUFFER_SIZE} steps on {DEVICE} ---")

# 3. æ¨¡æ‹Ÿæ•°æ®æ”¶é›† (ä¸€ä¸ª rollout)
for _ in range(BUFFER_SIZE):
    # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ® (åŒ…æ‹¬ value å’Œ log_prob)
    obs = torch.randn(NUM_ENVS, *OBS_SHAPE)
    action = torch.randn(NUM_ENVS, *ACTION_SHAPE)
    reward = torch.randn(NUM_ENVS)
    done = torch.zeros(NUM_ENVS, dtype=torch.bool)
    value = torch.randn(NUM_ENVS)
    log_prob = torch.randn(NUM_ENVS)

    rollout_buffer.add(obs, action, reward, done, value, log_prob)

print("Rollout collection complete.")

# 4. è®¡ç®— Advantages å’Œ Returns
# æ¨¡æ‹Ÿè·å–æœ€åä¸€ä¸ªçŠ¶æ€çš„ value estimate
last_values = torch.randn(NUM_ENVS)
last_dones = torch.zeros(NUM_ENVS, dtype=torch.bool)
rollout_buffer.compute_returns_and_advantages(last_values, last_dones)

print("\n--- Simulating training loop for PPO ---")
# 5. ä½¿ç”¨ç”Ÿæˆå™¨è¿›è¡Œå¤šè½®æ¬¡è®­ç»ƒ
for epoch in range(N_EPOCHS):
    batch_count = 0
    # get() æ–¹æ³•è¿”å›ä¸€ä¸ªç”Ÿæˆå™¨
    for batch in rollout_buffer.get(batch_size=BATCH_SIZE):
        # åœ¨è¿™é‡Œæ‰§è¡Œä½ çš„è®­ç»ƒæ­¥éª¤...
        # train_on_batch(batch)
        batch_count += 1
    
    print(f"Epoch {epoch + 1}: Trained on {batch_count} mini-batches.")

# 6. æŸ¥çœ‹ç»Ÿè®¡ç»“æœ
print("\n--- Episode Statistics ---")
stats = stats_tracker.get_statistics()
if not stats:
    print("No complete episodes during this short rollout.")
else:
    for key, value in stats.items():
        print(f"{key}: {value:.4f}")

```

## ğŸ› ï¸ å¼€å‘ä¸æµ‹è¯•

è¦è¿è¡Œé¡¹ç›®é™„å¸¦çš„å•å…ƒæµ‹è¯•ï¼Œè¯·ç¡®ä¿å·²å®‰è£… `dev` ä¾èµ–ï¼Œç„¶åæ‰§è¡Œï¼š

```bash
pytest
```

## å±•æœ›

æˆ‘ä»¬è®¡åˆ’åœ¨æœªæ¥ç»§ç»­å¢å¼º `rl-buffer` çš„åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š

-   [ ] **ä¼˜å…ˆç»éªŒå›æ”¾ (PER)**
-   [ ] **åŸç”Ÿæ”¯æŒå­—å…¸è§‚æµ‹ç©ºé—´ (`Dict Spaces`)**
-   [ ] **æ”¯æŒå¾ªç¯ç­–ç•¥ (Recurrent Policies) çš„åºåˆ—ç¼“å†²**

æ¬¢è¿è´¡çŒ®å’Œæå‡ºå»ºè®®ï¼

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ [MIT License](LICENSE)ã€‚